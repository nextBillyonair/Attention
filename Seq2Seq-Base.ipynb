{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.seq2seq import *\n",
    "from src.attention import *\n",
    "from src.utils import *\n",
    "from src.layers import MaskedCrossEntropyLoss\n",
    "import torch \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONS: \n",
    "# ENGLISH - en, \n",
    "# GERMAN - de, \n",
    "# FRENCH - fr, \n",
    "# CZECH - cs\n",
    "\n",
    "lang1 = 'de'\n",
    "lang2 = 'en'\n",
    "\n",
    "# train_sentences, test_sentences = load_data(lang1, lang2)\n",
    "# train_sentences = (train_sentences[0][:3000], train_sentences[1][:3000])\n",
    "train_sentences = load_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE=0.2\n",
    "BATCH_SIZE=64\n",
    "VALID_BATCH_SIZE=64\n",
    "MAX_VOCAB=20000\n",
    "\n",
    "src_vocab, tgt_vocab, train_loader, valid_loader = make_dataset(train_sentences, train_sentences, BATCH_SIZE, VALID_BATCH_SIZE, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 2000\n",
      "Number of testing examples: 2000\n",
      "Training Batches 32\tValidation Batches 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of testing examples: {len(valid_loader.dataset)}\")\n",
    "print(f\"Training Batches {len(train_loader)}\\tValidation Batches {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 6132\n",
      "Unique tokens in target (en) vocabulary: 3100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source ({lang1}) vocabulary: {len(src_vocab)}\")\n",
    "print(f\"Unique tokens in target ({lang2}) vocabulary: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER ARGS\n",
    "ENC_UNITS = 128\n",
    "ENC_EMBEDDING = 256\n",
    "SRC_VOCAB_SIZE = len(src_vocab)\n",
    "ENC_NUM_LAYERS = 1\n",
    "\n",
    "# DECODER ARGS\n",
    "DEC_UNITS = ENC_UNITS\n",
    "DEC_EMBEDDING = ENC_EMBEDDING\n",
    "TGT_VOCAB_SIZE = len(tgt_vocab)\n",
    "DEC_NUM_LAYERS = ENC_NUM_LAYERS\n",
    "\n",
    "# SEQ2SEQ ARGS\n",
    "TEACHER_FORCING = 1.0\n",
    "MAX_LENGTH = train_loader.dataset.tensors[1].size(-1) + 1\n",
    "SOS_TOKEN = tgt_vocab.SOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,059,740 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(ENC_UNITS, ENC_EMBEDDING, SRC_VOCAB_SIZE, ENC_NUM_LAYERS)\n",
    "decoder = Decoder(DEC_UNITS, DEC_EMBEDDING, TGT_VOCAB_SIZE, DEC_NUM_LAYERS)\n",
    "\n",
    "seq2seq = Seq2Seq(encoder, decoder, TEACHER_FORCING, MAX_LENGTH, SOS_TOKEN)\n",
    "\n",
    "print(f'The model has {count_parameters(seq2seq):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(6132, 256)\n",
      "    (gru): GRU(256, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(3100, 256)\n",
      "    (gru): GRU(256, 128, batch_first=True)\n",
      "    (fc): Linear(in_features=128, out_features=3100, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MaskedCrossEntropyLoss(pad_tok=tgt_vocab.PAD_token)\n",
    "optimizer = optim.Adam(seq2seq.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab.SOS_token, tgt_vocab.EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 16.31it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_loss = evaluate(seq2seq, valid_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.052558198571205"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.76it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.45it/s]\n",
      "\tTrain Loss: 5.162 | Train PPL: 174.546\n",
      "\t Val. Loss: 5.053 |  Val. PPL: 156.557\n",
      "Epoch: 02\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.71it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.44it/s]\n",
      "\tTrain Loss: 5.085 | Train PPL: 161.517\n",
      "\t Val. Loss: 4.978 |  Val. PPL: 145.135\n",
      "Epoch: 03\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.69it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.91it/s]\n",
      "\tTrain Loss: 5.004 | Train PPL: 149.074\n",
      "\t Val. Loss: 4.908 |  Val. PPL: 135.386\n",
      "Epoch: 04\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.47it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n",
      "\tTrain Loss: 4.936 | Train PPL: 139.232\n",
      "\t Val. Loss: 4.816 |  Val. PPL: 123.435\n",
      "Epoch: 05\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.65it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.41it/s]\n",
      "\tTrain Loss: 4.867 | Train PPL: 129.941\n",
      "\t Val. Loss: 4.757 |  Val. PPL: 116.438\n",
      "Epoch: 06\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.58it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.55it/s]\n",
      "\tTrain Loss: 4.795 | Train PPL: 120.949\n",
      "\t Val. Loss: 4.678 |  Val. PPL: 107.561\n",
      "Epoch: 07\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.62it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.49it/s]\n",
      "\tTrain Loss: 4.715 | Train PPL: 111.573\n",
      "\t Val. Loss: 4.598 |  Val. PPL:  99.332\n",
      "Epoch: 08\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.60it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.45it/s]\n",
      "\tTrain Loss: 4.643 | Train PPL: 103.893\n",
      "\t Val. Loss: 4.519 |  Val. PPL:  91.782\n",
      "Epoch: 09\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.51it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.43it/s]\n",
      "\tTrain Loss: 4.548 | Train PPL:  94.485\n",
      "\t Val. Loss: 4.449 |  Val. PPL:  85.522\n",
      "Epoch: 10\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.51it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.26it/s]\n",
      "\tTrain Loss: 4.499 | Train PPL:  89.937\n",
      "\t Val. Loss: 4.390 |  Val. PPL:  80.623\n",
      "Epoch: 11\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.50it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.39it/s]\n",
      "\tTrain Loss: 4.423 | Train PPL:  83.388\n",
      "\t Val. Loss: 4.330 |  Val. PPL:  75.942\n",
      "Epoch: 12\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n",
      "\tTrain Loss: 4.353 | Train PPL:  77.687\n",
      "\t Val. Loss: 4.246 |  Val. PPL:  69.810\n",
      "Epoch: 13\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.38it/s]\n",
      "\tTrain Loss: 4.289 | Train PPL:  72.925\n",
      "\t Val. Loss: 4.179 |  Val. PPL:  65.279\n",
      "Epoch: 14\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.18it/s]\n",
      "\tTrain Loss: 4.228 | Train PPL:  68.572\n",
      "\t Val. Loss: 4.123 |  Val. PPL:  61.745\n",
      "Epoch: 15\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.51it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.20it/s]\n",
      "\tTrain Loss: 4.168 | Train PPL:  64.589\n",
      "\t Val. Loss: 4.055 |  Val. PPL:  57.689\n",
      "Epoch: 16\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.54it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.20it/s]\n",
      "\tTrain Loss: 4.100 | Train PPL:  60.364\n",
      "\t Val. Loss: 3.991 |  Val. PPL:  54.104\n",
      "Epoch: 17\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.05it/s]\n",
      "100%|██████████| 32/32 [00:02<00:00, 15.10it/s]\n",
      "\tTrain Loss: 4.031 | Train PPL:  56.291\n",
      "\t Val. Loss: 3.925 |  Val. PPL:  50.658\n",
      "Epoch: 18\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.57it/s]\n",
      "\tTrain Loss: 3.971 | Train PPL:  53.031\n",
      "\t Val. Loss: 3.878 |  Val. PPL:  48.310\n",
      "Epoch: 19\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.53it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.30it/s]\n",
      "\tTrain Loss: 3.908 | Train PPL:  49.778\n",
      "\t Val. Loss: 3.799 |  Val. PPL:  44.665\n",
      "Epoch: 20\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.53it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n",
      "\tTrain Loss: 3.847 | Train PPL:  46.845\n",
      "\t Val. Loss: 3.747 |  Val. PPL:  42.406\n",
      "Epoch: 21\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.45it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.78it/s]\n",
      "\tTrain Loss: 3.787 | Train PPL:  44.110\n",
      "\t Val. Loss: 3.676 |  Val. PPL:  39.506\n",
      "Epoch: 22\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.52it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.00it/s]\n",
      "\tTrain Loss: 3.727 | Train PPL:  41.574\n",
      "\t Val. Loss: 3.607 |  Val. PPL:  36.848\n",
      "Epoch: 23\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n",
      "\tTrain Loss: 3.659 | Train PPL:  38.825\n",
      "\t Val. Loss: 3.551 |  Val. PPL:  34.855\n",
      "Epoch: 24\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.55it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.08it/s]\n",
      "\tTrain Loss: 3.611 | Train PPL:  37.014\n",
      "\t Val. Loss: 3.509 |  Val. PPL:  33.427\n",
      "Epoch: 25\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.53it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.15it/s]\n",
      "\tTrain Loss: 3.553 | Train PPL:  34.934\n",
      "\t Val. Loss: 3.437 |  Val. PPL:  31.094\n",
      "Epoch: 26\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.54it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.28it/s]\n",
      "\tTrain Loss: 3.493 | Train PPL:  32.899\n",
      "\t Val. Loss: 3.385 |  Val. PPL:  29.513\n",
      "Epoch: 27\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.54it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n",
      "\tTrain Loss: 3.447 | Train PPL:  31.420\n",
      "\t Val. Loss: 3.334 |  Val. PPL:  28.063\n",
      "Epoch: 28\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.26it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.86it/s]\n",
      "\tTrain Loss: 3.385 | Train PPL:  29.514\n",
      "\t Val. Loss: 3.277 |  Val. PPL:  26.509\n",
      "Epoch: 29\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.50it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.73it/s]\n",
      "\tTrain Loss: 3.343 | Train PPL:  28.297\n",
      "\t Val. Loss: 3.232 |  Val. PPL:  25.338\n",
      "Epoch: 30\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.52it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.25it/s]\n",
      "\tTrain Loss: 3.276 | Train PPL:  26.466\n",
      "\t Val. Loss: 3.178 |  Val. PPL:  23.992\n",
      "Epoch: 31\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.50it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.14it/s]\n",
      "\tTrain Loss: 3.231 | Train PPL:  25.295\n",
      "\t Val. Loss: 3.130 |  Val. PPL:  22.871\n",
      "Epoch: 32\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.52it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.33it/s]\n",
      "\tTrain Loss: 3.168 | Train PPL:  23.766\n",
      "\t Val. Loss: 3.062 |  Val. PPL:  21.362\n",
      "Epoch: 33\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.55it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.20it/s]\n",
      "\tTrain Loss: 3.133 | Train PPL:  22.945\n",
      "\t Val. Loss: 3.018 |  Val. PPL:  20.454\n",
      "Epoch: 34\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.54it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.13it/s]\n",
      "\tTrain Loss: 3.069 | Train PPL:  21.528\n",
      "\t Val. Loss: 2.977 |  Val. PPL:  19.634\n",
      "Epoch: 35\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.53it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.21it/s]\n",
      "\tTrain Loss: 3.033 | Train PPL:  20.767\n",
      "\t Val. Loss: 2.915 |  Val. PPL:  18.446\n",
      "Epoch: 36\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.34it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.51it/s]\n",
      "\tTrain Loss: 2.976 | Train PPL:  19.618\n",
      "\t Val. Loss: 2.870 |  Val. PPL:  17.629\n",
      "Epoch: 37\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.53it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.09it/s]\n",
      "\tTrain Loss: 2.942 | Train PPL:  18.951\n",
      "\t Val. Loss: 2.831 |  Val. PPL:  16.962\n",
      "Epoch: 38\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.50it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.76it/s]\n",
      "\tTrain Loss: 2.882 | Train PPL:  17.846\n",
      "\t Val. Loss: 2.786 |  Val. PPL:  16.215\n",
      "Epoch: 39\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.52it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.03it/s]\n",
      "\tTrain Loss: 2.836 | Train PPL:  17.047\n",
      "\t Val. Loss: 2.747 |  Val. PPL:  15.590\n",
      "Epoch: 40\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.53it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.06it/s]\n",
      "\tTrain Loss: 2.809 | Train PPL:  16.597\n",
      "\t Val. Loss: 2.699 |  Val. PPL:  14.872\n",
      "Epoch: 41\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.51it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.09it/s]\n",
      "\tTrain Loss: 2.751 | Train PPL:  15.662\n",
      "\t Val. Loss: 2.655 |  Val. PPL:  14.224\n",
      "Epoch: 42\n",
      "100%|██████████| 32/32 [00:12<00:00,  3.51it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.09it/s]\n",
      "\tTrain Loss: 2.709 | Train PPL:  15.021\n",
      "\t Val. Loss: 2.601 |  Val. PPL:  13.472\n",
      "Epoch: 43\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.55it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.11it/s]\n",
      "\tTrain Loss: 2.657 | Train PPL:  14.260\n",
      "\t Val. Loss: 2.578 |  Val. PPL:  13.174\n",
      "Epoch: 44\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.54it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.18it/s]\n",
      "\tTrain Loss: 2.632 | Train PPL:  13.899\n",
      "\t Val. Loss: 2.528 |  Val. PPL:  12.524\n",
      "Epoch: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  3.58it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.92it/s]\n",
      "\tTrain Loss: 2.580 | Train PPL:  13.194\n",
      "\t Val. Loss: 2.474 |  Val. PPL:  11.867\n",
      "Epoch: 46\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.00it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 16.32it/s]\n",
      "\tTrain Loss: 2.508 | Train PPL:  12.284\n",
      "\t Val. Loss: 2.434 |  Val. PPL:  11.410\n",
      "Epoch: 47\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.24it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.09it/s]\n",
      "\tTrain Loss: 2.474 | Train PPL:  11.873\n",
      "\t Val. Loss: 2.395 |  Val. PPL:  10.969\n",
      "Epoch: 48\n",
      "100%|██████████| 32/32 [00:11<00:00,  2.59it/s]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.88it/s]\n",
      "\tTrain Loss: 2.435 | Train PPL:  11.415\n",
      "\t Val. Loss: 2.340 |  Val. PPL:  10.382\n",
      "Epoch: 49\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.56it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.56it/s]\n",
      "\tTrain Loss: 2.415 | Train PPL:  11.193\n",
      "\t Val. Loss: 2.318 |  Val. PPL:  10.154\n",
      "Epoch: 50\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.57it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.52it/s]\n",
      "\tTrain Loss: 2.367 | Train PPL:  10.661\n",
      "\t Val. Loss: 2.269 |  Val. PPL:   9.671\n",
      "Epoch: 51\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.55it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.59it/s]\n",
      "\tTrain Loss: 2.322 | Train PPL:  10.194\n",
      "\t Val. Loss: 2.230 |  Val. PPL:   9.298\n",
      "Epoch: 52\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.33it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.17it/s]\n",
      "\tTrain Loss: 2.295 | Train PPL:   9.927\n",
      "\t Val. Loss: 2.186 |  Val. PPL:   8.901\n",
      "Epoch: 53\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.57it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.38it/s]\n",
      "\tTrain Loss: 2.245 | Train PPL:   9.442\n",
      "\t Val. Loss: 2.156 |  Val. PPL:   8.632\n",
      "Epoch: 54\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.43it/s]\n",
      "\tTrain Loss: 2.192 | Train PPL:   8.956\n",
      "\t Val. Loss: 2.109 |  Val. PPL:   8.240\n",
      "Epoch: 55\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.70it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.67it/s]\n",
      "\tTrain Loss: 2.148 | Train PPL:   8.564\n",
      "\t Val. Loss: 2.089 |  Val. PPL:   8.077\n",
      "Epoch: 56\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.66it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.06it/s]\n",
      "\tTrain Loss: 2.124 | Train PPL:   8.366\n",
      "\t Val. Loss: 2.040 |  Val. PPL:   7.691\n",
      "Epoch: 57\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.51it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.94it/s]\n",
      "\tTrain Loss: 2.075 | Train PPL:   7.963\n",
      "\t Val. Loss: 1.994 |  Val. PPL:   7.346\n",
      "Epoch: 58\n",
      "100%|██████████| 32/32 [00:10<00:00,  2.90it/s]\n",
      "100%|██████████| 32/32 [00:02<00:00, 13.55it/s]\n",
      "\tTrain Loss: 2.056 | Train PPL:   7.815\n",
      "\t Val. Loss: 1.948 |  Val. PPL:   7.017\n",
      "Epoch: 59\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.57it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.40it/s]\n",
      "\tTrain Loss: 2.020 | Train PPL:   7.538\n",
      "\t Val. Loss: 1.944 |  Val. PPL:   6.984\n",
      "Epoch: 60\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.61it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.54it/s]\n",
      "\tTrain Loss: 1.982 | Train PPL:   7.260\n",
      "\t Val. Loss: 1.885 |  Val. PPL:   6.586\n",
      "Epoch: 61\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.59it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.48it/s]\n",
      "\tTrain Loss: 1.938 | Train PPL:   6.947\n",
      "\t Val. Loss: 1.870 |  Val. PPL:   6.486\n",
      "Epoch: 62\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.59it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.17it/s]\n",
      "\tTrain Loss: 1.919 | Train PPL:   6.813\n",
      "\t Val. Loss: 1.825 |  Val. PPL:   6.200\n",
      "Epoch: 63\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.58it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.38it/s]\n",
      "\tTrain Loss: 1.880 | Train PPL:   6.551\n",
      "\t Val. Loss: 1.802 |  Val. PPL:   6.061\n",
      "Epoch: 64\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.59it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.34it/s]\n",
      "\tTrain Loss: 1.848 | Train PPL:   6.347\n",
      "\t Val. Loss: 1.753 |  Val. PPL:   5.772\n",
      "Epoch: 65\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.57it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.13it/s]\n",
      "\tTrain Loss: 1.811 | Train PPL:   6.115\n",
      "\t Val. Loss: 1.725 |  Val. PPL:   5.613\n",
      "Epoch: 66\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.30it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.03it/s]\n",
      "\tTrain Loss: 1.778 | Train PPL:   5.916\n",
      "\t Val. Loss: 1.686 |  Val. PPL:   5.397\n",
      "Epoch: 67\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.53it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.32it/s]\n",
      "\tTrain Loss: 1.731 | Train PPL:   5.649\n",
      "\t Val. Loss: 1.651 |  Val. PPL:   5.214\n",
      "Epoch: 68\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.74it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.94it/s]\n",
      "\tTrain Loss: 1.708 | Train PPL:   5.518\n",
      "\t Val. Loss: 1.645 |  Val. PPL:   5.179\n",
      "Epoch: 69\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.74it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.58it/s]\n",
      "\tTrain Loss: 1.683 | Train PPL:   5.381\n",
      "\t Val. Loss: 1.575 |  Val. PPL:   4.830\n",
      "Epoch: 70\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.60it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.35it/s]\n",
      "\tTrain Loss: 1.656 | Train PPL:   5.237\n",
      "\t Val. Loss: 1.556 |  Val. PPL:   4.737\n",
      "Epoch: 71\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.52it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.39it/s]\n",
      "\tTrain Loss: 1.625 | Train PPL:   5.079\n",
      "\t Val. Loss: 1.524 |  Val. PPL:   4.591\n",
      "Epoch: 72\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.56it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.19it/s]\n",
      "\tTrain Loss: 1.561 | Train PPL:   4.763\n",
      "\t Val. Loss: 1.468 |  Val. PPL:   4.340\n",
      "Epoch: 73\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.59it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.21it/s]\n",
      "\tTrain Loss: 1.526 | Train PPL:   4.598\n",
      "\t Val. Loss: 1.454 |  Val. PPL:   4.279\n",
      "Epoch: 74\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.61it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.21it/s]\n",
      "\tTrain Loss: 1.496 | Train PPL:   4.463\n",
      "\t Val. Loss: 1.444 |  Val. PPL:   4.236\n",
      "Epoch: 75\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.59it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.24it/s]\n",
      "\tTrain Loss: 1.463 | Train PPL:   4.319\n",
      "\t Val. Loss: 1.389 |  Val. PPL:   4.010\n",
      "Epoch: 76\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.48it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.24it/s]\n",
      "\tTrain Loss: 1.433 | Train PPL:   4.190\n",
      "\t Val. Loss: 1.354 |  Val. PPL:   3.875\n",
      "Epoch: 77\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.72it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.77it/s]\n",
      "\tTrain Loss: 1.384 | Train PPL:   3.990\n",
      "\t Val. Loss: 1.320 |  Val. PPL:   3.743\n",
      "Epoch: 78\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.73it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.44it/s]\n",
      "\tTrain Loss: 1.360 | Train PPL:   3.897\n",
      "\t Val. Loss: 1.304 |  Val. PPL:   3.682\n",
      "Epoch: 79\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.72it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.70it/s]\n",
      "\tTrain Loss: 1.345 | Train PPL:   3.837\n",
      "\t Val. Loss: 1.272 |  Val. PPL:   3.567\n",
      "Epoch: 80\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.56it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.11it/s]\n",
      "\tTrain Loss: 1.307 | Train PPL:   3.694\n",
      "\t Val. Loss: 1.229 |  Val. PPL:   3.417\n",
      "Epoch: 81\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.56it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.10it/s]\n",
      "\tTrain Loss: 1.269 | Train PPL:   3.557\n",
      "\t Val. Loss: 1.204 |  Val. PPL:   3.333\n",
      "Epoch: 82\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.52it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.93it/s]\n",
      "\tTrain Loss: 1.234 | Train PPL:   3.434\n",
      "\t Val. Loss: 1.154 |  Val. PPL:   3.170\n",
      "Epoch: 83\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.56it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.05it/s]\n",
      "\tTrain Loss: 1.220 | Train PPL:   3.388\n",
      "\t Val. Loss: 1.141 |  Val. PPL:   3.129\n",
      "Epoch: 84\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.58it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 16.48it/s]\n",
      "\tTrain Loss: 1.198 | Train PPL:   3.312\n",
      "\t Val. Loss: 1.112 |  Val. PPL:   3.042\n",
      "Epoch: 85\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.56it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.14it/s]\n",
      "\tTrain Loss: 1.166 | Train PPL:   3.210\n",
      "\t Val. Loss: 1.082 |  Val. PPL:   2.949\n",
      "Epoch: 86\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.55it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.28it/s]\n",
      "\tTrain Loss: 1.116 | Train PPL:   3.053\n",
      "\t Val. Loss: 1.058 |  Val. PPL:   2.882\n",
      "Epoch: 87\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.55it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.15it/s]\n",
      "\tTrain Loss: 1.092 | Train PPL:   2.981\n",
      "\t Val. Loss: 1.037 |  Val. PPL:   2.821\n",
      "Epoch: 88\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.57it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.17it/s]\n",
      "\tTrain Loss: 1.078 | Train PPL:   2.938\n",
      "\t Val. Loss: 1.011 |  Val. PPL:   2.748\n",
      "Epoch: 89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  3.77it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.55it/s]\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.885\n",
      "\t Val. Loss: 0.995 |  Val. PPL:   2.703\n",
      "Epoch: 90\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.73it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.59it/s]\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.848\n",
      "\t Val. Loss: 0.968 |  Val. PPL:   2.633\n",
      "Epoch: 91\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.59it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.85it/s]\n",
      "\tTrain Loss: 0.992 | Train PPL:   2.697\n",
      "\t Val. Loss: 0.936 |  Val. PPL:   2.550\n",
      "Epoch: 92\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.60it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.95it/s]\n",
      "\tTrain Loss: 0.967 | Train PPL:   2.629\n",
      "\t Val. Loss: 0.913 |  Val. PPL:   2.492\n",
      "Epoch: 93\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.61it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.93it/s]\n",
      "\tTrain Loss: 0.935 | Train PPL:   2.548\n",
      "\t Val. Loss: 0.890 |  Val. PPL:   2.434\n",
      "Epoch: 94\n",
      "100%|██████████| 32/32 [00:11<00:00,  3.14it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 17.78it/s]\n",
      "\tTrain Loss: 0.904 | Train PPL:   2.469\n",
      "\t Val. Loss: 0.867 |  Val. PPL:   2.380\n",
      "Epoch: 95\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.59it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.62it/s]\n",
      "\tTrain Loss: 0.903 | Train PPL:   2.467\n",
      "\t Val. Loss: 0.845 |  Val. PPL:   2.327\n",
      "Epoch: 96\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.58it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.69it/s]\n",
      "\tTrain Loss: 0.866 | Train PPL:   2.378\n",
      "\t Val. Loss: 0.826 |  Val. PPL:   2.283\n",
      "Epoch: 97\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.73it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.07it/s]\n",
      "\tTrain Loss: 0.844 | Train PPL:   2.326\n",
      "\t Val. Loss: 0.785 |  Val. PPL:   2.193\n",
      "Epoch: 98\n",
      "100%|██████████| 32/32 [00:09<00:00,  3.75it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 19.09it/s]\n",
      "\tTrain Loss: 0.814 | Train PPL:   2.256\n",
      "\t Val. Loss: 0.750 |  Val. PPL:   2.117\n",
      "Epoch: 99\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.60it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.10it/s]\n",
      "\tTrain Loss: 0.789 | Train PPL:   2.202\n",
      "\t Val. Loss: 0.735 |  Val. PPL:   2.086\n",
      "Epoch: 100\n",
      "100%|██████████| 32/32 [00:10<00:00,  3.61it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 18.00it/s]\n",
      "\tTrain Loss: 0.760 | Train PPL:   2.139\n",
      "\t Val. Loss: 0.716 |  Val. PPL:   2.046\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "\n",
    "seq2seq.teacher_forcing = 0.\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    \n",
    "    train_loss = train(seq2seq, train_loader, optimizer, criterion, CLIP, src_vocab.PAD_token)\n",
    "    valid_loss = evaluate(seq2seq, valid_loader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), 'models/seq2seq.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.load_state_dict(torch.load('models/seq2seq.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "\n",
    "src_sentence = train_loader.dataset.tensors[0][idx:idx+1]\n",
    "tgt_sentence = train_loader.dataset.tensors[1][idx:idx+1]\n",
    "\n",
    "src_sentence = src_vocab.to_string(src_sentence, remove_special=True)[0]\n",
    "tgt_sentence = tgt_vocab.to_string(tgt_sentence, remove_special=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   4,  100,  389,   14,  191,  675, 1063, 3670,  390,  391,  389,   19,\n",
      "            9, 3671,  414,    9, 3672,   29,  191, 3673,   12,  960,   29, 3674,\n",
      "         3675,   10,  415,    3,    5]])\n",
      "tensor([[1497, 1498, 1073,  752, 1499,   79,  662,  662,  906,  522,    5,  522,\n",
      "            3,    3,    3,    3]])\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate(src_sentence, seq2seq, src_vocab, tgt_vocab, src_vocab.PAD_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> south korea s nuclear envoy kim sook urged north korea monday to restart work to disable its nuclear plants and stop its typical brinkmanship in negotiations .\n",
      "= envoy urges north korea to restart nuclear disablement\n",
      "< aga khan pours his wealth into islamic islamic sites syria in syria\n"
     ]
    }
   ],
   "source": [
    "print(f\"> {src_sentence}\")\n",
    "print(f\"= {tgt_sentence}\")\n",
    "print(f\"< {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
