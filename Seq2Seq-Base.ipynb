{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.seq2seq import *\n",
    "from src.attention import *\n",
    "from src.utils import *\n",
    "from src.layers import MaskedCrossEntropyLoss\n",
    "import torch \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONS: \n",
    "# ENGLISH - en, \n",
    "# GERMAN - de, \n",
    "# FRENCH - fr, \n",
    "# CZECH - cs\n",
    "\n",
    "lang1 = 'de'\n",
    "lang2 = 'en'\n",
    "\n",
    "train_sentences, test_sentences = load_data(lang1, lang2)\n",
    "train_sentences = (train_sentences[0][:3000], train_sentences[1][:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE=0.2\n",
    "BATCH_SIZE=64\n",
    "VALID_BATCH_SIZE=128\n",
    "MAX_VOCAB=20000\n",
    "\n",
    "src_vocab, tgt_vocab, train_loader, valid_loader = make_dataset(train_sentences, test_sentences, BATCH_SIZE, VALID_BATCH_SIZE, MAX_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 3000\n",
      "Number of testing examples: 1014\n",
      "Training Batches 47\tValidation Batches 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of testing examples: {len(valid_loader.dataset)}\")\n",
    "print(f\"Training Batches {len(train_loader)}\\tValidation Batches {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 4389\n",
      "Unique tokens in target (en) vocabulary: 3446\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source ({lang1}) vocabulary: {len(src_vocab)}\")\n",
    "print(f\"Unique tokens in target ({lang2}) vocabulary: {len(tgt_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODER ARGS\n",
    "ENC_UNITS = 128\n",
    "ENC_EMBEDDING = 128\n",
    "SRC_VOCAB_SIZE = len(src_vocab)\n",
    "ENC_NUM_LAYERS = 1\n",
    "\n",
    "# DECODER ARGS\n",
    "DEC_UNITS = ENC_UNITS\n",
    "DEC_EMBEDDING = ENC_EMBEDDING\n",
    "TGT_VOCAB_SIZE = len(tgt_vocab)\n",
    "DEC_NUM_LAYERS = ENC_NUM_LAYERS\n",
    "\n",
    "# SEQ2SEQ ARGS\n",
    "TEACHER_FORCING = 1.0\n",
    "MAX_LENGTH = train_loader.dataset.tensors[1].size(-1) + 1\n",
    "SOS_TOKEN = tgt_vocab.SOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,645,558 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(ENC_UNITS, ENC_EMBEDDING, SRC_VOCAB_SIZE, ENC_NUM_LAYERS)\n",
    "decoder = Decoder(DEC_UNITS, DEC_EMBEDDING, TGT_VOCAB_SIZE, DEC_NUM_LAYERS)\n",
    "\n",
    "seq2seq = Seq2Seq(encoder, decoder, TEACHER_FORCING, MAX_LENGTH, SOS_TOKEN)\n",
    "\n",
    "print(f'The model has {count_parameters(seq2seq):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(4389, 128)\n",
      "    (gru): GRU(128, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(3446, 128)\n",
      "    (gru): GRU(128, 128, batch_first=True)\n",
      "    (fc): Linear(in_features=128, out_features=3446, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MaskedCrossEntropyLoss(pad_tok=tgt_vocab.PAD_token)\n",
    "optimizer = optim.Adam(seq2seq.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab.SOS_token, tgt_vocab.EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:05<00:00,  9.24it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_loss = evaluate(seq2seq, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.154829877488156"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "100%|██████████| 47/47 [00:36<00:00,  1.31it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.87it/s]\n",
      "\tTrain Loss: 6.132 | Train PPL: 460.402\n",
      "\t Val. Loss: 5.972 |  Val. PPL: 392.134\n",
      "Epoch: 02\n",
      "100%|██████████| 47/47 [00:35<00:00,  1.32it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.88it/s]\n",
      "\tTrain Loss: 4.750 | Train PPL: 115.632\n",
      "\t Val. Loss: 5.763 |  Val. PPL: 318.387\n",
      "Epoch: 03\n",
      "100%|██████████| 47/47 [00:36<00:00,  1.31it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.87it/s]\n",
      "\tTrain Loss: 4.431 | Train PPL:  84.056\n",
      "\t Val. Loss: 5.998 |  Val. PPL: 402.788\n",
      "Epoch: 04\n",
      "100%|██████████| 47/47 [00:36<00:00,  1.31it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.65it/s]\n",
      "\tTrain Loss: 4.181 | Train PPL:  65.452\n",
      "\t Val. Loss: 6.017 |  Val. PPL: 410.359\n",
      "Epoch: 05\n",
      "100%|██████████| 47/47 [00:36<00:00,  1.32it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.81it/s]\n",
      "\tTrain Loss: 3.983 | Train PPL:  53.652\n",
      "\t Val. Loss: 6.176 |  Val. PPL: 480.838\n",
      "Epoch: 06\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.28it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.97it/s]\n",
      "\tTrain Loss: 3.821 | Train PPL:  45.657\n",
      "\t Val. Loss: 6.231 |  Val. PPL: 508.370\n",
      "Epoch: 07\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.32it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.80it/s]\n",
      "\tTrain Loss: 3.680 | Train PPL:  39.661\n",
      "\t Val. Loss: 6.304 |  Val. PPL: 546.662\n",
      "Epoch: 08\n",
      "100%|██████████| 47/47 [00:36<00:00,  1.46it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.24it/s]\n",
      "\tTrain Loss: 3.553 | Train PPL:  34.926\n",
      "\t Val. Loss: 6.442 |  Val. PPL: 627.617\n",
      "Epoch: 09\n",
      "100%|██████████| 47/47 [00:35<00:00,  1.32it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.80it/s]\n",
      "\tTrain Loss: 3.438 | Train PPL:  31.127\n",
      "\t Val. Loss: 6.530 |  Val. PPL: 685.678\n",
      "Epoch: 10\n",
      "100%|██████████| 47/47 [00:37<00:00,  1.31it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.82it/s]\n",
      "\tTrain Loss: 3.331 | Train PPL:  27.952\n",
      "\t Val. Loss: 6.637 |  Val. PPL: 763.114\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    \n",
    "    train_loss = train(seq2seq, train_loader, optimizer, criterion, CLIP, src_vocab.PAD_token)\n",
    "    valid_loss = evaluate(seq2seq, valid_loader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq.state_dict(), 'models/seq2seq.pt')\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.load_state_dict(torch.load('models/seq2seq.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 80\n",
    "\n",
    "src_sentence = train_loader.dataset.tensors[0][idx:idx+1]\n",
    "tgt_sentence = train_loader.dataset.tensors[1][idx:idx+1]\n",
    "\n",
    "src_sentence = src_vocab.to_string(src_sentence, remove_special=True)[0]\n",
    "tgt_sentence = tgt_vocab.to_string(tgt_sentence, remove_special=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,   17,   43,   32, 1766,  125,   67,   21,    7, 1767,   75,    4,\n",
      "            3]])\n",
      "tensor([[3445, 3299,  644,  369, 2197,  125, 1753,  166, 2735, 2706, 2551, 2706,\n",
      "         2768, 2768,  422,  713,  713, 3022, 1201, 3293, 2665,  121, 2727,  997,\n",
      "         2958,  240, 3225, 2200,   23, 3052, 1393,   28, 2872, 2138,  755, 2158,\n",
      "         2388, 2715, 2715]])\n"
     ]
    }
   ],
   "source": [
    "translation, attention = translate(src_sentence, seq2seq, src_vocab, tgt_vocab, src_vocab.PAD_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> zwei gro e lohfarbene hunde spielen an einem sandigen strand .\n",
      "= two large tan dogs play along a sandy beach .\n",
      "< snowdrift snaps station couch adjusting hair tae older college sparrow measured sparrow code code dancing practicing practicing waited biking hulk pointed holds slushy moving gardens sleeping conditioner slope young raise doors dog mop-pad 12 tied wakeboarding flea culture culture\n"
     ]
    }
   ],
   "source": [
    "print(f\"> {src_sentence}\")\n",
    "print(f\"= {tgt_sentence}\")\n",
    "print(f\"< {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
