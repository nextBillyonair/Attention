# Attention
Wrappers for Attention and Transformer/RNN Models

# Attention Layers
  * Concat (Bahadanu) Attention
  * Dot (From Luong) Attention
  * Mean Attention
  * Last In Sequence Attention

# Extra Activation Functions
  * GELU

# Layers
  * Batch First Wrapper for Transformer Encoder

# Models
  * RNNModel - RNN layer + Attention
  * Transformer
