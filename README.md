# Attention
Wrappers for Attention and Transformer/RNN Models

# Positional Embedding Layers
  * Sine Embeddings
  * Cosine Embeddings
  * Positional Embeddings (Sine on even, Cosine on Odd, see Attention is All You Need)

# Attention Layers
  * Concat (Bahadanu) Attention
  * Dot (From Luong) Attention
  * Mean Attention
  * Last In Sequence Attention
  * Batch First Multihead Attention

# Activation Functions
  * GELU

# Layers
  * Batch First Wrapper for Transformer Encoder

# Models - Showcase
  * RNNModel - RNN layer + Attention
  * Transformer with Positional Embeddings
